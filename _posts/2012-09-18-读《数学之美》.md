---
layout: default
title: 读《数学之美》
---
花半天时间拜读了吴军的[数学之美](https://sites.google.com/site/junwu02/beautyofmathematics)系列文章，总体感觉不像是关于数学的书，倒是像关于数学怎么应用到计算机科学的故事。如果你期待更多关于数学的内容，这个可能让你失望了（比如我就失望了），但是如果修了跟计算机相关的课程，做做课前预习还是可以滴~ 不过说回来，非常佩服作者的功力以及总结能力，如果不是理解到了炉火纯青的地步，也不能写出这种大众科普性质的文章来。  
### [Page Rank](http://www.google.com.hk/ggblog/googlechinablog/2006/02/page-rank-google_1386.html) ###
貌似没啥好说的，太熟悉了。  
###[统计语言模型](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_7327.html)###
这个方法很妙~  
> 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为：  
> `P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1)`  
> 其中 `P(w1)` 表示第一个词w1 出现的概率；`P (w2|w1)` 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词 `wn`，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 `wi-1` 有关(即马尔可夫假设），于是问题就变得很简单了。现在，`S` 出现的概率就变为：  
> `P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)…`  
> (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。）  
> 接下来的问题就是如何估计 `P(wi|wi-1)`。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。 
 
###[中文分词](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_2507.html)###
分词这个事情，用汉语的很好理解，我感觉去理解一句话的话，是这样：句读 → 意思。句读怎么句读呢，就是之前很多词语/短语的积累。所以计算机分词要分的准，就要积累许多词，就是要有个大词典去查啦。  
但实际上，干这个活远没有这么简单，详情请见我看的几个中文分词的开源项目：[折腾分词](http://vivizhyy.github.com/2012/09/19/%E6%8A%98%E8%85%BE-ICTCLA-%E5%88%86%E8%AF%8D%E4%BB%A5%E5%8F%8A%E5%88%86%E8%AF%8D.html)
  
###[隐含马尔可夫模型](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_1583.html)###
这篇文档对 HMM 介绍有限，建议阅读 [沙克](http://www.suzker.cn) 的以下 3 篇文章：
<ul>
<li> <a href="http://www.suzker.cn/computervision/the-hmm-model.html">隐马科夫(HMM)模型</a>: 通过一个框装水果的比喻来介绍 HMM 的三个问题： <ol><li>给定一个观察到得序列O，及参数lambda，求出P(O|lambda)，即发生这种观察序列的可能性。对应上面例子中，即是我给定一个最终确定了的水果序列S，求我选到这样的水果的可能性。</li>
<li>给定一个观察到的序列O，及参数lambda，求出最有可能产生这种序列的状态序列S。对应上面例子中，即我给定一个最终确定了的水果序列S，求我最可能的选水果筐路径。</li>
<li>同样给定一个观察得到的序列O，求如何调整参数lambda，使P(O|lambda)最大。</li>
</ol>  
</li>
<li> <a href="http://www.suzker.cn/computervision/forward-agorithm-for-hmm.html">HMM中的前向法(Forward Agorithm)</a>: 解决上面 HMM 模型的第一个问题，用一个递归方法减少重复计算，妙极。</li>
<li><a href="http://www.suzker.cn/computervision/viterbi-algorithm-for-hmm.html/4">HMM中的维特比解码(Viterbi Agorithm)</a>:解决上面 HMM 的第二个问题，也是 <a href="http://ictclas.org/">ICTCLAS</a> 用到的分词算法。</li>
</ul>
  
###[怎样度量信息?](http://www.google.com.hk/ggblog/googlechinablog/2006/04/4_1731.html)###
关于信息熵的介绍。简单的说，熵是用来度量一个物体内部混乱程度的，越混乱，熵越大。对于信息熵来说，熵越大，表示包含的不同信息越多（越“混乱”），信息量越大。
  
###[简单之美：布尔代数和搜索引擎的索引](http://www.google.com.hk/ggblog/googlechinablog/2006/05/blog-post_3044.html)###
比如搜“刘德华歌曲”  
- 分词  
- 把有 “刘德华” 和 “歌曲” 的网页做个列表  
- 两个列表相 与，结果返回（二进制操作）  
  
###[图论和网络爬虫 (Web Crawlers)](http://www.google.com.hk/ggblog/googlechinablog/2006/05/web-crawlers_73.html)###
网页之间通过链接，组成一个图，爬虫可以通过这些链接关系，遍历这个网络图。该文档为什么没有介绍这个著名的 bow tie 捏...
<img src="http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/%22Bow-tie%22_diagram_of_components_in_a_directed_network.jpg/964px-%22Bow-tie%22_diagram_of_components_in_a_directed_network.jpg" width=580/>  
  
###[信息论在信息处理中的应用](http://www.google.com.hk/ggblog/googlechinablog/2006/05/blog-post_2403.html)###
信息论中 3 个重要的概念：
<ul>
<li>熵： <strong>衡量统计语言模型的好坏。</strong>一个模型的复杂度越小，模型越好。</li>
<li>“互信息”（Mutual Information): <strong>两个随机事件相关性的度量。</strong>比如说今天随机事件北京下雨和随机变量空气湿度的相关性就很大，但是和姚明所在的休斯敦火箭队是否能赢公牛队几乎无关。互信息就是用来量化度量这种相关性的。</li>
<li>“相对熵”（Kullback-Leibler Divergence)：<strong>衡量两个正函数是否相似，对于两个完全相同的函数，它们的相对熵等于零。</strong>在自然语言处理中可以用相对熵来衡量两个常用词（在语法上和语义上）是否同义，或者两篇文章的内容是否相近等等。利用相对熵，我们可以到处信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。
</ul>
  
###[贾里尼克的故事和现代语言处理](http://www.google.com.hk/ggblog/googlechinablog/2006/06/blog-post_2074.html)###
八卦，感兴趣的话猛击点击进去看就行了。  
  
###[有限状态机和地址识别](http://www.google.com.hk/ggblog/googlechinablog/2006/07/blog-post_4950.html)###
![](http://www.kuqin.com/upimg/allimg/071204/2113080.jpg)  
  
###[Google 阿卡 47 的制造者阿米特.辛格博士](http://www.google.com.hk/ggblog/googlechinablog/2006/07/google-47_3471.html)  
八卦，直接猛击进去看就好了。  
  
###[余弦定理和新闻的分类](http://www.google.com.hk/ggblog/googlechinablog/2006/07/12_4010.html)###
从实际工作来讲，这个帖子写的有点坑爹。
  
###[信息指纹及其应用](http://www.google.com.hk/ggblog/googlechinablog/2006/08/blog-post_8115.html)###
把一长串的信息编码成一小节，并且不能被解码。  
  
###[谈谈数学模型的重要性](http://www.google.com.hk/ggblog/googlechinablog/2006/08/blog-post_2006.html)###
<ol>
<li>一个正确的数学模型应当在形式上是简单的。</li>
<li>一个正确的模型在它开始的时候可能还不如一个精雕细琢过的错误的模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去。</li>
<li>大量准确的数据对研发很重要。</li>
<li>正确的模型也可能受噪音干扰，而显得不准确；这时我们不应该用一种凑合的修正方法来弥补它，而是要找到噪音的根源，这也许能通往重大发现。</li>
</ol>
  
###[自然语言处理的几位精英](http://www.google.com.hk/ggblog/googlechinablog/2006/08/blog-post_6232.html)###
八卦不解释。
  
###[最大熵模型](http://www.google.com.hk/ggblog/googlechinablog/2006/10/blog-post_2150.html)###
一个拼音转汉字的简单的例子。我们已知两种信息，第一，根据语言模型，wang-xiao-bo 可以被转换成王晓波和王小波；第二，根据主题，王小波是作家，《黄金时代》的作者等等，而王晓波是台湾研究两岸关系的学者。因此，我们就可以建立一个最大熵模型，同时满足这两种信息。下面公式是根据上下文（前两个词）和主题预测下一个词的最大熵模型，其中 w3 是要预测的词（王晓波或者王小波）w1 和 w2 是它的前两个字（比如说它们分别是“出版”，和“”），也就是其上下文的一个大致估计，subject 表示主题。
![](http://images.cnblogs.com/cnblogs_com/KevinYang/WindowsLiveWriter/a96dd33f7281_B6C5/image_2.png)  
在上面的公式中，有几个参数 lambda 和 Z ，他们需要通过观测数据训练出来。  
最大熵模型在数学上十分完美，但是计算收敛很复杂。  
  
###[闪光的不一定是金子 谈谈搜索引擎作弊问题(Search Engine Anti-SPAM)](http://www.google.com.hk/ggblog/googlechinablog/2006/11/search-engine-anti-spam_9453.html)###
![](http://www.sowang.com/add/math-782969.jpg)  
在图中，原始的信号混入了噪音，在数学上相当于两个信号做卷积。噪音消除的过程是一个解卷积的过程。这在信号处理中并不是什么难题。因为第一，汽车发动机的频率是固定的，第二，这个频率的噪音重复出现，只要采集几秒钟的信号进行处理就能做到。从广义上讲，只要噪音不是完全随机的、并且前后有相关性，就可以检测到并且消除。（事实上，完全随机不相关的高斯白噪音是很难消除的。）  
搜索引擎的作弊者所作的事，就如同在手机信号中加入了噪音，使得搜索结果的排名完全乱了。但是，这种人为加入的噪音并不难消除，因为作弊者的方法不可能是随机的（否则就无法提高排名了）。而且，作弊者也不可能是一天换一种方法，即作弊方法是时间相关的。因此，搞搜索引擎排名算法的人，可以在搜集一段时间的作弊信息后，将作弊者抓出来，还原原有的排名。当然这个过程需要时间，就如同采集汽车发动机噪音需要时间一样，在这段时间内，作弊者可能会尝到些甜头。  
  
###[矩阵运算和文本处理中的分类问题](http://www.google.com.hk/ggblog/googlechinablog/2006/12/blog-post_8935.html)###
SVD 分解。  
  
###[自然语言处理的教父 马库斯](http://www.google.com.hk/ggblog/googlechinablog/2007/04/blog-post_6253.html)###
八卦  
  
###[马尔可夫链的扩展 贝叶斯网络 (Bayesian Networks)](http://www.google.com.hk/ggblog/googlechinablog/2007/01/bayesian-networks_997.html)###
马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。  
和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。  
  
###[由电视剧《暗算》所想到的 — 谈谈密码学的数学原理](http://www.google.com.hk/ggblog/googlechinablog/2007/09/blog-post_5853.html)###
![](http://www.kuqin.com/upimg/allimg/071204/2235033.jpg)  
所有的密码用暴力都是可以破解的，问题是时间长短。  
  
###[输入一个汉字需要敲多少个键 — 谈谈香农第一定律](http://www.google.com.hk/ggblog/googlechinablog/2007/12/blog-post_7986.html)
作为一个坚持认为理论应该为指导实践铺路，如果理论在实践中完全不可行那么不用看了的人来说，这篇帖子可以完全忽略。