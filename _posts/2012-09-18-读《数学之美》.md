---
layout: default
title: 读《数学之美》
---
花半天时间拜读了吴军的[数学之美](https://sites.google.com/site/junwu02/beautyofmathematics)系列文章，总体感觉不像是关于数学的书，倒是像关于数学怎么应用到计算机科学的故事。如果你期待更多关于数学的内容，这个可能让你失望了（比如我就失望了），但是如果修了跟计算机相关的课程，做做课前预习还是可以滴~ 不过说回来，非常佩服作者的功力以及总结能力，如果不是理解到了炉火纯青的地步，也不能写出这种大众科普性质的文章来。  
### [Page Rank](http://www.google.com.hk/ggblog/googlechinablog/2006/02/page-rank-google_1386.html) ###
貌似没啥好说的，太熟悉了。  
###[统计语言模型](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_7327.html)###
这个方法很妙~  
> 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为：  
> `P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1)`  
> 其中 `P(w1)` 表示第一个词w1 出现的概率；`P (w2|w1)` 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词 `wn`，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 `wi-1` 有关(即马尔可夫假设），于是问题就变得很简单了。现在，`S` 出现的概率就变为：  
> `P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)…`  
> (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。）  
> 接下来的问题就是如何估计 `P(wi|wi-1)`。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。 
 
###[中文分词](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_2507.html)###
分词这个事情，用汉语的很好理解，我感觉去理解一句话的话，是这样：句读 → 意思。句读怎么句读呢，就是之前很多词语/短语的积累。所以计算机分词要分的准，就要积累许多词，就是要有个大词典去查啦。  
  
###[隐含马尔可夫模型](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_1583.html)###
好吧，我觉得这篇文档最有价值的地方是介绍了 [Viterbi](http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s2_pg1.html) 算法，但是我觉得这个最有价值的地方竟然只给了个链接。